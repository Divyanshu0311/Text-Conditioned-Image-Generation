{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! gdown --id 10H5s1fUPmas3DhgnIoxXUD-5FWLvZs2F\n",
        "! gdown --id 1ZparTI41rixCABwRlU52PRZQ5HEADfGm\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYaHIs1E1in7",
        "outputId": "11549f33-f4de-4ec6-952d-2382cb2ea174"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10H5s1fUPmas3DhgnIoxXUD-5FWLvZs2F\n",
            "To: /content/mnist_train.csv.zip\n",
            "100% 13.7M/13.7M [00:00<00:00, 101MB/s] \n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZparTI41rixCABwRlU52PRZQ5HEADfGm\n",
            "To: /content/mnist_test.csv.zip\n",
            "100% 2.28M/2.28M [00:00<00:00, 159MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip mnist_train.csv.zip\n",
        "!unzip mnist_test.csv.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIpIo7wO2bqs",
        "outputId": "e2e6215e-7089-4559-ad6a-948170912cc1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  mnist_train.csv.zip\n",
            "  inflating: mnist_train.csv         \n",
            "Archive:  mnist_test.csv.zip\n",
            "  inflating: mnist_test.csv          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5xM8o9Qz0gS",
        "outputId": "d55a3e1c-18dc-4f26-9c90-290de59f5ef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished creating 1001 images in data/train/images\n",
            "Finished creating 2001 images in data/train/images\n",
            "Finished creating 3001 images in data/train/images\n",
            "Finished creating 4001 images in data/train/images\n",
            "Finished creating 5001 images in data/train/images\n",
            "Finished creating 6001 images in data/train/images\n",
            "Finished creating 7001 images in data/train/images\n",
            "Finished creating 8001 images in data/train/images\n",
            "Finished creating 9001 images in data/train/images\n",
            "Finished creating 10001 images in data/train/images\n",
            "Finished creating 11001 images in data/train/images\n",
            "Finished creating 12001 images in data/train/images\n",
            "Finished creating 13001 images in data/train/images\n",
            "Finished creating 14001 images in data/train/images\n",
            "Finished creating 15001 images in data/train/images\n",
            "Finished creating 16001 images in data/train/images\n",
            "Finished creating 17001 images in data/train/images\n",
            "Finished creating 18001 images in data/train/images\n",
            "Finished creating 19001 images in data/train/images\n",
            "Finished creating 20001 images in data/train/images\n",
            "Finished creating 21001 images in data/train/images\n",
            "Finished creating 22001 images in data/train/images\n",
            "Finished creating 23001 images in data/train/images\n",
            "Finished creating 24001 images in data/train/images\n",
            "Finished creating 25001 images in data/train/images\n",
            "Finished creating 26001 images in data/train/images\n",
            "Finished creating 27001 images in data/train/images\n",
            "Finished creating 28001 images in data/train/images\n",
            "Finished creating 29001 images in data/train/images\n",
            "Finished creating 30001 images in data/train/images\n",
            "Finished creating 31001 images in data/train/images\n",
            "Finished creating 32001 images in data/train/images\n",
            "Finished creating 33001 images in data/train/images\n",
            "Finished creating 34001 images in data/train/images\n",
            "Finished creating 35001 images in data/train/images\n",
            "Finished creating 36001 images in data/train/images\n",
            "Finished creating 37001 images in data/train/images\n",
            "Finished creating 38001 images in data/train/images\n",
            "Finished creating 39001 images in data/train/images\n",
            "Finished creating 40001 images in data/train/images\n",
            "Finished creating 41001 images in data/train/images\n",
            "Finished creating 42001 images in data/train/images\n",
            "Finished creating 43001 images in data/train/images\n",
            "Finished creating 44001 images in data/train/images\n",
            "Finished creating 45001 images in data/train/images\n",
            "Finished creating 46001 images in data/train/images\n",
            "Finished creating 47001 images in data/train/images\n",
            "Finished creating 48001 images in data/train/images\n",
            "Finished creating 49001 images in data/train/images\n",
            "Finished creating 50001 images in data/train/images\n",
            "Finished creating 51001 images in data/train/images\n",
            "Finished creating 52001 images in data/train/images\n",
            "Finished creating 53001 images in data/train/images\n",
            "Finished creating 54001 images in data/train/images\n",
            "Finished creating 55001 images in data/train/images\n",
            "Finished creating 56001 images in data/train/images\n",
            "Finished creating 57001 images in data/train/images\n",
            "Finished creating 58001 images in data/train/images\n",
            "Finished creating 59001 images in data/train/images\n",
            "Finished creating 60001 images in data/train/images\n",
            "Finished creating 1001 images in data/test/images\n",
            "Finished creating 2001 images in data/test/images\n",
            "Finished creating 3001 images in data/test/images\n",
            "Finished creating 4001 images in data/test/images\n",
            "Finished creating 5001 images in data/test/images\n",
            "Finished creating 6001 images in data/test/images\n",
            "Finished creating 7001 images in data/test/images\n",
            "Finished creating 8001 images in data/test/images\n",
            "Finished creating 9001 images in data/test/images\n",
            "Finished creating 10001 images in data/test/images\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import _csv as csv\n",
        "\n",
        "def extract_images(save_dir, csv_fname):\n",
        "    assert os.path.exists(save_dir), \"Directory {} to save images does not exist\".format(save_dir)\n",
        "    assert os.path.exists(csv_fname), \"Csv file {} does not exist\".format(csv_fname)\n",
        "    with open(csv_fname) as f:\n",
        "        reader = csv.reader(f)\n",
        "        for idx, row in enumerate(reader):\n",
        "            if idx == 0:\n",
        "                continue\n",
        "            im = np.zeros((784))\n",
        "            im[:] = list(map(int, row[1:]))\n",
        "            im = im.reshape((28,28))\n",
        "            if not os.path.exists(os.path.join(save_dir, row[0])):\n",
        "                os.mkdir(os.path.join(save_dir, row[0]))\n",
        "            cv2.imwrite(os.path.join(save_dir, row[0], '{}.png'.format(idx)), im)\n",
        "            if idx % 1000 == 0:\n",
        "                print('Finished creating {} images in {}'.format(idx+1, save_dir))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    extract_images('data/train/images', 'mnist_train.csv')\n",
        "    extract_images('data/test/images', 'mnist_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class LinearNoiseScheduler:\n",
        "    def __init__(self, num_timesteps, beta_start, beta_end):\n",
        "        self.num_timesteps = num_timesteps\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_end = beta_end\n",
        "\n",
        "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
        "        self.alphas = 1. - self.betas\n",
        "        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n",
        "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
        "        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alpha_cum_prod)\n",
        "\n",
        "    def add_noise(self, original, noise, t):\n",
        "        original_shape = original.shape\n",
        "        batch_size = original_shape[0]\n",
        "\n",
        "        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n",
        "        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod.to(original.device)[t].reshape(batch_size)\n",
        "\n",
        "        for _ in range(len(original_shape) - 1):\n",
        "            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n",
        "        for _ in range(len(original_shape) - 1):\n",
        "            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n",
        "\n",
        "        return (sqrt_alpha_cum_prod.to(original.device) * original\n",
        "                + sqrt_one_minus_alpha_cum_prod.to(original.device) * noise)\n",
        "\n",
        "    def sample_prev_timestep(self, xt, noise_pred, t):\n",
        "        x0 = ((xt - (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t] * noise_pred)) /\n",
        "              torch.sqrt(self.alpha_cum_prod.to(xt.device)[t]))\n",
        "        x0 = torch.clamp(x0, -1., 1.)\n",
        "\n",
        "        mean = xt - ((self.betas.to(xt.device)[t]) * noise_pred) / (self.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t])\n",
        "        mean = mean / torch.sqrt(self.alphas.to(xt.device)[t])\n",
        "\n",
        "        if t == 0:\n",
        "            return mean, x0\n",
        "        else:\n",
        "            variance = (1 - self.alpha_cum_prod.to(xt.device)[t - 1]) / (1.0 - self.alpha_cum_prod.to(xt.device)[t])\n",
        "            variance = variance * self.betas.to(xt.device)[t]\n",
        "            sigma = variance ** 0.5\n",
        "            z = torch.randn(xt.shape).to(xt.device)\n",
        "\n",
        "            return mean + sigma * z, x0"
      ],
      "metadata": {
        "id": "aKd6Og-p5f31"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_time_embedding(time_steps, temb_dim):\n",
        "\n",
        "    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n",
        "\n",
        "    factor = 10000 ** ((torch.arange(\n",
        "        start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2))\n",
        "    )\n",
        "\n",
        "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
        "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
        "    return t_emb"
      ],
      "metadata": {
        "id": "R_xMlqEeCLXc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
        "                 down_sample=True, num_heads=4, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.down_sample = down_sample\n",
        "        self.resnet_conv_first = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n",
        "                              kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.t_emb_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(t_emb_dim, out_channels)\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.resnet_conv_second = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(8, out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(out_channels, out_channels,\n",
        "                              kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.attention_norms = nn.ModuleList(\n",
        "            [nn.GroupNorm(8, out_channels)\n",
        "             for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.attentions = nn.ModuleList(\n",
        "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "             for _ in range(num_layers)]\n",
        "        )\n",
        "        self.residual_input_conv = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n",
        "                                          4, 2, 1) if self.down_sample else nn.Identity()\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "        out = x\n",
        "        for i in range(self.num_layers):\n",
        "\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i](out)\n",
        "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i](out)\n",
        "            out = out + self.residual_input_conv[i](resnet_input)\n",
        "\n",
        "            batch_size, channels, h, w = out.shape\n",
        "            in_attn = out.reshape(batch_size, channels, h * w)\n",
        "            in_attn = self.attention_norms[i](in_attn)\n",
        "            in_attn = in_attn.transpose(1, 2)\n",
        "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "            out = out + out_attn\n",
        "\n",
        "        out = self.down_sample_conv(out)\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zeLfIOwOCM-I"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MidBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads=4, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.resnet_conv_first = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
        "                              padding=1),\n",
        "                )\n",
        "                for i in range(num_layers+1)\n",
        "            ]\n",
        "        )\n",
        "        self.t_emb_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(t_emb_dim, out_channels)\n",
        "            )\n",
        "            for _ in range(num_layers + 1)\n",
        "        ])\n",
        "        self.resnet_conv_second = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(8, out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "                for _ in range(num_layers+1)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.attention_norms = nn.ModuleList(\n",
        "            [nn.GroupNorm(8, out_channels)\n",
        "                for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.attentions = nn.ModuleList(\n",
        "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "                for _ in range(num_layers)]\n",
        "        )\n",
        "        self.residual_input_conv = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
        "                for i in range(num_layers+1)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "        out = x\n",
        "\n",
        "        resnet_input = out\n",
        "        out = self.resnet_conv_first[0](out)\n",
        "        out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
        "        out = self.resnet_conv_second[0](out)\n",
        "        out = out + self.residual_input_conv[0](resnet_input)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "\n",
        "            batch_size, channels, h, w = out.shape\n",
        "            in_attn = out.reshape(batch_size, channels, h * w)\n",
        "            in_attn = self.attention_norms[i](in_attn)\n",
        "            in_attn = in_attn.transpose(1, 2)\n",
        "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "            out = out + out_attn\n",
        "\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i+1](out)\n",
        "            out = out + self.t_emb_layers[i+1](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i+1](out)\n",
        "            out = out + self.residual_input_conv[i+1](resnet_input)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "yMshtVuvBBxi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample=True, num_heads=4, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.up_sample = up_sample\n",
        "        self.resnet_conv_first = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(8, in_channels if i == 0 else out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
        "                              padding=1),\n",
        "                )\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.t_emb_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(t_emb_dim, out_channels)\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.resnet_conv_second = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.GroupNorm(8, out_channels),\n",
        "                    nn.SiLU(),\n",
        "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.attention_norms = nn.ModuleList(\n",
        "            [\n",
        "                nn.GroupNorm(8, out_channels)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.attentions = nn.ModuleList(\n",
        "            [\n",
        "                nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.residual_input_conv = nn.ModuleList(\n",
        "            [\n",
        "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2,\n",
        "                                                 4, 2, 1) \\\n",
        "            if self.up_sample else nn.Identity()\n",
        "\n",
        "    def forward(self, x, out_down, t_emb):\n",
        "        x = self.up_sample_conv(x)\n",
        "        x = torch.cat([x, out_down], dim=1)\n",
        "\n",
        "        out = x\n",
        "        for i in range(self.num_layers):\n",
        "            resnet_input = out\n",
        "            out = self.resnet_conv_first[i](out)\n",
        "            out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
        "            out = self.resnet_conv_second[i](out)\n",
        "            out = out + self.residual_input_conv[i](resnet_input)\n",
        "\n",
        "            batch_size, channels, h, w = out.shape\n",
        "            in_attn = out.reshape(batch_size, channels, h * w)\n",
        "            in_attn = self.attention_norms[i](in_attn)\n",
        "            in_attn = in_attn.transpose(1, 2)\n",
        "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
        "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "            out = out + out_attn\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "IL9UryD_CfWK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unet(nn.Module):\n",
        "    def __init__(self, model_config):\n",
        "        super().__init__()\n",
        "        im_channels = model_config['im_channels']\n",
        "        self.down_channels = model_config['down_channels']\n",
        "        self.mid_channels = model_config['mid_channels']\n",
        "        self.t_emb_dim = model_config['time_emb_dim']\n",
        "        self.down_sample = model_config['down_sample']\n",
        "        self.num_down_layers = model_config['num_down_layers']\n",
        "        self.num_mid_layers = model_config['num_mid_layers']\n",
        "        self.num_up_layers = model_config['num_up_layers']\n",
        "\n",
        "        assert self.mid_channels[0] == self.down_channels[-1]\n",
        "        assert self.mid_channels[-1] == self.down_channels[-2]\n",
        "        assert len(self.down_sample) == len(self.down_channels) - 1\n",
        "\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
        "        )\n",
        "\n",
        "        self.up_sample = list(reversed(self.down_sample))\n",
        "        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=(1, 1))\n",
        "\n",
        "        self.downs = nn.ModuleList([])\n",
        "        for i in range(len(self.down_channels)-1):\n",
        "            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i+1], self.t_emb_dim,\n",
        "                                        down_sample=self.down_sample[i], num_layers=self.num_down_layers))\n",
        "\n",
        "        self.mids = nn.ModuleList([])\n",
        "        for i in range(len(self.mid_channels)-1):\n",
        "            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i+1], self.t_emb_dim,\n",
        "                                      num_layers=self.num_mid_layers))\n",
        "\n",
        "        self.ups = nn.ModuleList([])\n",
        "        for i in reversed(range(len(self.down_channels)-1)):\n",
        "            self.ups.append(UpBlock(self.down_channels[i] * 2, self.down_channels[i-1] if i != 0 else 16,\n",
        "                                    self.t_emb_dim, up_sample=self.down_sample[i], num_layers=self.num_up_layers))\n",
        "\n",
        "        self.norm_out = nn.GroupNorm(8, 16)\n",
        "        self.conv_out = nn.Conv2d(16, im_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        out = self.conv_in(x)\n",
        "        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n",
        "        t_emb = self.t_proj(t_emb)\n",
        "\n",
        "        down_outs = []\n",
        "\n",
        "        for idx, down in enumerate(self.downs):\n",
        "            down_outs.append(out)\n",
        "            out = down(out, t_emb)\n",
        "        for mid in self.mids:\n",
        "            out = mid(out, t_emb)\n",
        "\n",
        "        for up in self.ups:\n",
        "            down_out = down_outs.pop()\n",
        "            out = up(out, down_out, t_emb)\n",
        "        out = self.norm_out(out)\n",
        "        out = nn.SiLU()(out)\n",
        "        out = self.conv_out(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "y-Hju2T6CkGM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "\n",
        "class MnistDataset(Dataset):\n",
        "    def __init__(self, split, im_path, im_ext='png'):\n",
        "        self.split = split\n",
        "        self.im_ext = im_ext\n",
        "        self.images, self.labels = self.load_images(im_path)\n",
        "\n",
        "    def load_images(self, im_path):\n",
        "        assert os.path.exists(im_path), \"images path {} does not exist\".format(im_path)\n",
        "        ims = []\n",
        "        labels = []\n",
        "        for d_name in tqdm(os.listdir(im_path)):\n",
        "            for fname in glob.glob(os.path.join(im_path, d_name, '*.{}'.format(self.im_ext))):\n",
        "                ims.append(fname)\n",
        "                labels.append(int(d_name))\n",
        "        print('Found {} images for split {}'.format(len(ims), self.split))\n",
        "        return ims, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        im = Image.open(self.images[index])\n",
        "        im_tensor = torchvision.transforms.ToTensor()(im)\n",
        "\n",
        "        im_tensor = (2 * im_tensor) - 1\n",
        "        return im_tensor"
      ],
      "metadata": {
        "id": "UqFeP-6n54W7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import yaml\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def train(args):\n",
        "    with open(args.config_path, 'r') as file:\n",
        "        try:\n",
        "            config = yaml.safe_load(file)\n",
        "        except yaml.YAMLError as exc:\n",
        "            print(exc)\n",
        "    print(config)\n",
        "    ####################\n",
        "    diffusion_config = config['diffusion_params']\n",
        "    dataset_config = config['dataset_params']\n",
        "    model_config = config['model_params']\n",
        "    train_config = config['train_params']\n",
        "\n",
        "    scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'],\n",
        "                                     beta_start=diffusion_config['beta_start'],\n",
        "                                     beta_end=diffusion_config['beta_end'])\n",
        "\n",
        "    mnist = MnistDataset('train', im_path=dataset_config['im_path'])\n",
        "    mnist_loader = DataLoader(mnist, batch_size=train_config['batch_size'], shuffle=True, num_workers=4)\n",
        "\n",
        "    model = Unet(model_config).to(device)\n",
        "    model.train()\n",
        "\n",
        "    if not os.path.exists(train_config['task_name']):\n",
        "        os.mkdir(train_config['task_name'])\n",
        "\n",
        "    if os.path.exists(os.path.join(train_config['task_name'],train_config['ckpt_name'])):\n",
        "        print('Loading checkpoint as found one')\n",
        "        model.load_state_dict(torch.load(os.path.join(train_config['task_name'],\n",
        "                                                      train_config['ckpt_name']), map_location=device))\n",
        "    num_epochs = train_config['num_epochs']\n",
        "    optimizer = Adam(model.parameters(), lr=train_config['lr'])\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    for epoch_idx in range(num_epochs):\n",
        "        losses = []\n",
        "        for im in tqdm(mnist_loader):\n",
        "            optimizer.zero_grad()\n",
        "            im = im.float().to(device)\n",
        "\n",
        "            noise = torch.randn_like(im).to(device)\n",
        "\n",
        "            t = torch.randint(0, diffusion_config['num_timesteps'], (im.shape[0],)).to(device)\n",
        "\n",
        "            noisy_im = scheduler.add_noise(im, noise, t)\n",
        "            noise_pred = model(noisy_im, t)\n",
        "\n",
        "            loss = criterion(noise_pred, noise)\n",
        "            losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print('Finished epoch:{} | Loss : {:.4f}'.format(\n",
        "            epoch_idx + 1,\n",
        "            np.mean(losses),\n",
        "        ))\n",
        "        torch.save(model.state_dict(), os.path.join(train_config['task_name'],\n",
        "                                                    train_config['ckpt_name']))\n",
        "\n",
        "    print('Done Training')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Arguments for ddpm training')\n",
        "    parser.add_argument('--config', dest='config_path',\n",
        "                        default='default.yaml', type=str)\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    train(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brQe68WaCtEj",
        "outputId": "c2c0fe22-165a-4226-e67d-1d493b38db3e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'dataset_params': {'im_path': 'data/train/images'}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.0001, 'beta_end': 0.02}, 'model_params': {'im_channels': 1, 'im_size': 28, 'down_channels': [32, 64, 128, 256], 'mid_channels': [256, 256, 128], 'down_sample': [True, True, False], 'time_emb_dim': 128, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2, 'num_heads': 4}, 'train_params': {'task_name': 'default', 'batch_size': 16, 'num_epochs': 4, 'num_samples': 100, 'num_grid_rows': 10, 'lr': 0.0001, 'ckpt_name': 'ddpm_ckpt.pth'}}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 74.30it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 60000 images for split train\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3750/3750 [06:12<00:00, 10.07it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished epoch:1 | Loss : 0.0556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3750/3750 [06:14<00:00, 10.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished epoch:2 | Loss : 0.0292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3750/3750 [06:14<00:00, 10.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished epoch:3 | Loss : 0.0270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3750/3750 [06:13<00:00, 10.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished epoch:4 | Loss : 0.0256\n",
            "Done Training ...\n"
          ]
        }
      ]
    }
  ]
}